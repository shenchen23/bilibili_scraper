{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SC_Scraper:\n",
    "\n",
    "    def __init__(self, debug=False, name = 'sc-scraper',platform = None):\n",
    "        self.name = name\n",
    "        self.debug = debug\n",
    "        self.store_identifier = None\n",
    "        self.platform = platform\n",
    "        self.driver = self.__get_driver()\n",
    "        #self.logger = self.__get_logger()\n",
    "        self.location = None\n",
    "        self.query = None\n",
    "        self.store_url_list = []\n",
    "        self.store_desc_list = []\n",
    "        self.store_url_available = []\n",
    "        self.store_list_url = None\n",
    "        self.num_of_stores = None\n",
    "        self.store_df = None\n",
    "        self.MAX_WAIT = 20\n",
    "        self.break_flag = False\n",
    "        self.header = ['query','location','store_id','store_str','store_score_total','id_review', 'caption', 'relative_date', 'retrieval_date', 'rating', 'username', 'n_review_user', 'n_photo_user', 'url_user']\n",
    "        \n",
    "        \n",
    "    def change_header(self,h):\n",
    "        self.header = h\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "        return True\n",
    "\n",
    "    def __scroll(self):\n",
    "        scrollable_div = self.driver.find_element_by_css_selector('div.section-layout.section-scrollbox.scrollable-y.scrollable-show')\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "    def __get_logger(self):\n",
    "        # create logger\n",
    "        logger = logging.getLogger(self.name)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create console handler and set level to debug\n",
    "        fh = logging.FileHandler('log/'+self.name+'.log')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        # create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        # add formatter to ch\n",
    "        fh.setFormatter(formatter)\n",
    "\n",
    "        # add ch to logger\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "        return logger\n",
    "    \n",
    "    def __loaded_check_by_classname(self,class_name):\n",
    "        try:\n",
    "            element = WebDriverWait(self.driver,15).until(EC.presence_of_element_located((By.CLASS_NAME,class_name)))\n",
    "        except:\n",
    "            self.logger.warn('Failed to move to page.')\n",
    "        return\n",
    "        \n",
    "    def __get_driver(self, debug=False):\n",
    "        options = Options()\n",
    "\n",
    "        if not self.debug:\n",
    "            options.add_argument(\"--headless\")\n",
    "        \n",
    "        options.add_argument(\"--window-size=1366,768\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--lang=en-GB\")\n",
    "        \n",
    "        if self.platform == 'Mobile':\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1')\n",
    "\n",
    "        input_driver = webdriver.Chrome('./chromedriver', options =options)\n",
    "\n",
    "        return input_driver\n",
    "\n",
    "\n",
    "    # util function to clean special characters\n",
    "    def __filter_string(self, str):\n",
    "        strOut = str.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "        return strOut\n",
    "    \n",
    "    def __scroll(self):\n",
    "        scrollable_div = self.driver.find_element_by_css_selector('div.section-layout.section-scrollbox.scrollable-y.scrollable-show')\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "    \n",
    "    def scroll_10(self):\n",
    "        k = 0\n",
    "        #self.logger.info('Scrolling loop...')\n",
    "        while k<10:\n",
    "            k += 1\n",
    "            self.__scroll()\n",
    "            time.sleep(2)\n",
    "\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def to_url(self,url):\n",
    "        \n",
    "        self.store_list_url = url\n",
    "        \n",
    "        self.driver.get(self.store_list_url)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def csv_writer(self,path='data/', outfile='gm_reviews.csv'):\n",
    "        try:\n",
    "            targetfile = open(path + outfile, mode='w', encoding='utf-8', newline='\\n')\n",
    "        except:\n",
    "            os.makedirs(path)\n",
    "            targetfile = open(path + outfile, mode='w', encoding='utf-8', newline='\\n')\n",
    "            \n",
    "        writer = csv.writer(targetfile, quoting=csv.QUOTE_MINIMAL)\n",
    "        h = self.header\n",
    "        writer.writerow(h)\n",
    "        return writer\n",
    "    \n",
    "    def __sc_parse(self,review):\n",
    "        item = {}\n",
    "        id_review = review.find('button', class_='section-review-action-menu')['data-review-id']\n",
    "        username = review.find('div', class_='section-review-title').find('span').text\n",
    "\n",
    "        try:\n",
    "            tmp_text = review.find('span', class_='section-review-text').tex\n",
    "\n",
    "            review_text = tmp_text.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            review_text = None\n",
    "\n",
    "        rating = float(review.find('span', class_='section-review-stars')['aria-label'].split(' ')[1])\n",
    "        relative_date = review.find('span', class_='section-review-publish-date').text\n",
    "\n",
    "        try:\n",
    "            n_reviews_photos = review.find('div', class_='section-review-subtitle').find_all('span')[1].text\n",
    "            metadata = n_reviews_photos.split('\\xe3\\x83\\xbb')\n",
    "            if len(metadata) == 3:\n",
    "                n_photos = int(metadata[2].split(' ')[0].replace('.', ''))\n",
    "            else:\n",
    "                n_photos = 0\n",
    "\n",
    "            idx = len(metadata)\n",
    "            n_reviews = int(metadata[idx - 1].split(' ')[0].replace('.', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            n_reviews = 0\n",
    "            n_photos = 0\n",
    "\n",
    "        user_url = review.find('a')['href']\n",
    "\n",
    "        item['id_review'] = id_review\n",
    "        item['caption'] = review_text\n",
    "\n",
    "            # depends on language, which depends on geolocation defined by Google Maps\n",
    "            # custom mapping to transform into date shuold be implemented\n",
    "        item['relative_date'] = relative_date\n",
    "\n",
    "            # store datetime of scraping and apply further processing to calculate\n",
    "            # correct date as retrieval_date - time(relative_date)\n",
    "        item['retrieval_date'] = datetime.now()\n",
    "        item['rating'] = rating\n",
    "        item['username'] = username\n",
    "        item['n_review_user'] = n_reviews\n",
    "        item['n_photo_user'] = n_photos\n",
    "        item['url_user'] = user_url\n",
    "\n",
    "        return item\n",
    " \n",
    "      \n",
    "\n",
    "import os\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "GM_WEBPAGE = 'https://www.google.com/maps/'\n",
    "MAX_WAIT = 10\n",
    "MAX_RETRY = 5\n",
    "MAX_SCROLLS = 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll():\n",
    "    for _ in range(25):\n",
    "        html = sc_scraper.driver.find_element_by_tag_name('html')\n",
    "        html.send_keys(Keys.END)\n",
    "        \n",
    "    time.sleep(2)\n",
    "    \n",
    "    for _ in range(25):\n",
    "        html = sc_scraper.driver.find_element_by_tag_name('html')\n",
    "        html.send_keys(Keys.END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc_scraper = SC_Scraper(debug = False, platform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc_scraper.to_url(\"https://www.bilibili.com/bangumi/play/ep302023\")\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_block = sc_scraper.driver.find_element_by_xpath('//*[@id=\"eplist_module\"]/div[2]/ul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_episode_list = episode_block.find_elements_by_xpath(\"./*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(single_episode_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_scraper.change_header(['user_id','user_homepage','level','timestamp','text','like','episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode_idx in range(len(single_episode_list)):\n",
    "    print(\"________________\")\n",
    "    print(episode_idx)\n",
    "\n",
    "    single_episode_list[episode_idx].click()\n",
    "    time.sleep(3)\n",
    "    scroll()\n",
    "    time.sleep(10)\n",
    "    page_number = int(sc_scraper.driver.find_element_by_xpath('//*[@id=\"comment_module\"]/div[2]/div/div[2]/div[2]/span[1]').text.replace(\"共\",'').replace(\"页\",''))\n",
    "\n",
    "    comment_writer = sc_scraper.csv_writer(path='data/', outfile=f'v0_episode_{str(episode_idx)}.csv')\n",
    "\n",
    "    for tmp_page in range(page_number):\n",
    "        print(f\"{tmp_page+1}/{page_number}\")\n",
    "        scroll()\n",
    "        page_inbox = sc_scraper.driver.find_element_by_class_name('page-jump').find_element_by_xpath('./input')\n",
    "        page_inbox.clear()\n",
    "        page_inbox.send_keys(int(tmp_page+1))\n",
    "        page_inbox.send_keys(Keys.RETURN)\n",
    "        scroll()\n",
    "        time.sleep(1)\n",
    "\n",
    "        comment_block = sc_scraper.driver.find_element_by_class_name('comment-list').find_elements_by_xpath(\"./*\")\n",
    "        for idx, tmp_comment in enumerate(comment_block):\n",
    "            text = tmp_comment.find_elements_by_class_name(\"text\")[0].text\n",
    "            try:\n",
    "                user_homepage = tmp_comment.find_elements_by_class_name(\"user\")[0].find_elements_by_xpath(\"./*\")[0].get_attribute('href')\n",
    "            except:\n",
    "                user_homepage = 'None'\n",
    "            try:\n",
    "                if user_homepage != 'None':\n",
    "                    user_id = user_homepage.split(\"/\")[-1]\n",
    "                else: \n",
    "                    user_id = 'None'\n",
    "            except:\n",
    "                user_id = 'None'\n",
    "            try:   \n",
    "                level = tmp_comment.find_elements_by_class_name(\"user\")[0].find_elements_by_class_name(\"level-link\")[0].find_elements_by_xpath(\"./*\")[0].get_attribute('class')\n",
    "            except:\n",
    "                level = 'None'\n",
    "            timestamp = tmp_comment.find_elements_by_class_name(\"info\")[0].find_elements_by_class_name(\"time\")[0].text\n",
    "            like = tmp_comment.find_elements_by_class_name(\"info\")[0].find_elements_by_class_name(\"like\")[0].text\n",
    "\n",
    "            row_val = [user_id,user_homepage,level,timestamp,text,like,episode_idx]\n",
    "            comment_writer.writerow(row_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
