{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SC_Scraper:\n",
    "\n",
    "    def __init__(self, debug=False, name = 'sc-scraper',platform = None):\n",
    "        self.name = name\n",
    "        self.debug = debug\n",
    "        self.store_identifier = None\n",
    "        self.platform = platform\n",
    "        self.driver = self.__get_driver()\n",
    "        #self.logger = self.__get_logger()\n",
    "        self.location = None\n",
    "        self.query = None\n",
    "        self.store_url_list = []\n",
    "        self.store_desc_list = []\n",
    "        self.store_url_available = []\n",
    "        self.store_list_url = None\n",
    "        self.num_of_stores = None\n",
    "        self.store_df = None\n",
    "        self.MAX_WAIT = 20\n",
    "        self.break_flag = False\n",
    "        self.header = ['query','location','store_id','store_str','store_score_total','id_review', 'caption', 'relative_date', 'retrieval_date', 'rating', 'username', 'n_review_user', 'n_photo_user', 'url_user']\n",
    "        \n",
    "        \n",
    "    def change_header(self,h):\n",
    "        self.header = h\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "        return True\n",
    "\n",
    "    def __scroll(self):\n",
    "        scrollable_div = self.driver.find_element_by_css_selector('div.section-layout.section-scrollbox.scrollable-y.scrollable-show')\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "    def __get_logger(self):\n",
    "        # create logger\n",
    "        logger = logging.getLogger(self.name)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create console handler and set level to debug\n",
    "        fh = logging.FileHandler('log/'+self.name+'.log')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        # create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        # add formatter to ch\n",
    "        fh.setFormatter(formatter)\n",
    "\n",
    "        # add ch to logger\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "        return logger\n",
    "    \n",
    "    def __loaded_check_by_classname(self,class_name):\n",
    "        try:\n",
    "            element = WebDriverWait(self.driver,15).until(EC.presence_of_element_located((By.CLASS_NAME,class_name)))\n",
    "        except:\n",
    "            self.logger.warn('Failed to move to page.')\n",
    "        return\n",
    "        \n",
    "    def __get_driver(self, debug=False):\n",
    "        options = Options()\n",
    "\n",
    "        if not self.debug:\n",
    "            options.add_argument(\"--headless\")\n",
    "        \n",
    "        options.add_argument(\"--window-size=1366,768\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--lang=en-GB\")\n",
    "        \n",
    "        if self.platform == 'Mobile':\n",
    "            options.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1')\n",
    "\n",
    "        input_driver = webdriver.Chrome('./chromedriver', options =options)\n",
    "\n",
    "        return input_driver\n",
    "\n",
    "\n",
    "    # util function to clean special characters\n",
    "    def __filter_string(self, str):\n",
    "        strOut = str.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "        return strOut\n",
    "    \n",
    "    def __scroll(self):\n",
    "        scrollable_div = self.driver.find_element_by_css_selector('div.section-layout.section-scrollbox.scrollable-y.scrollable-show')\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "    \n",
    "    def scroll_10(self):\n",
    "        k = 0\n",
    "        #self.logger.info('Scrolling loop...')\n",
    "        while k<10:\n",
    "            k += 1\n",
    "            self.__scroll()\n",
    "            time.sleep(2)\n",
    "\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def to_url(self,url):\n",
    "        \n",
    "        self.store_list_url = url\n",
    "        \n",
    "        self.driver.get(self.store_list_url)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def csv_writer(self,path='data/', outfile='gm_reviews.csv'):\n",
    "        try:\n",
    "            targetfile = open(path + outfile, mode='w', encoding='utf-8', newline='\\n')\n",
    "        except:\n",
    "            os.makedirs(path)\n",
    "            targetfile = open(path + outfile, mode='w', encoding='utf-8', newline='\\n')\n",
    "            \n",
    "        writer = csv.writer(targetfile, quoting=csv.QUOTE_MINIMAL)\n",
    "        h = self.header\n",
    "        writer.writerow(h)\n",
    "        return writer\n",
    "    \n",
    "    def __sc_parse(self,review):\n",
    "        item = {}\n",
    "        id_review = review.find('button', class_='section-review-action-menu')['data-review-id']\n",
    "        username = review.find('div', class_='section-review-title').find('span').text\n",
    "\n",
    "        try:\n",
    "            tmp_text = review.find('span', class_='section-review-text').tex\n",
    "\n",
    "            review_text = tmp_text.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            review_text = None\n",
    "\n",
    "        rating = float(review.find('span', class_='section-review-stars')['aria-label'].split(' ')[1])\n",
    "        relative_date = review.find('span', class_='section-review-publish-date').text\n",
    "\n",
    "        try:\n",
    "            n_reviews_photos = review.find('div', class_='section-review-subtitle').find_all('span')[1].text\n",
    "            metadata = n_reviews_photos.split('\\xe3\\x83\\xbb')\n",
    "            if len(metadata) == 3:\n",
    "                n_photos = int(metadata[2].split(' ')[0].replace('.', ''))\n",
    "            else:\n",
    "                n_photos = 0\n",
    "\n",
    "            idx = len(metadata)\n",
    "            n_reviews = int(metadata[idx - 1].split(' ')[0].replace('.', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            n_reviews = 0\n",
    "            n_photos = 0\n",
    "\n",
    "        user_url = review.find('a')['href']\n",
    "\n",
    "        item['id_review'] = id_review\n",
    "        item['caption'] = review_text\n",
    "\n",
    "            # depends on language, which depends on geolocation defined by Google Maps\n",
    "            # custom mapping to transform into date shuold be implemented\n",
    "        item['relative_date'] = relative_date\n",
    "\n",
    "            # store datetime of scraping and apply further processing to calculate\n",
    "            # correct date as retrieval_date - time(relative_date)\n",
    "        item['retrieval_date'] = datetime.now()\n",
    "        item['rating'] = rating\n",
    "        item['username'] = username\n",
    "        item['n_review_user'] = n_reviews\n",
    "        item['n_photo_user'] = n_photos\n",
    "        item['url_user'] = user_url\n",
    "\n",
    "        return item\n",
    " \n",
    "      \n",
    "\n",
    "import os\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "GM_WEBPAGE = 'https://www.google.com/maps/'\n",
    "MAX_WAIT = 10\n",
    "MAX_RETRY = 5\n",
    "MAX_SCROLLS = 40\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll():\n",
    "    for _ in range(25):\n",
    "        html = sc_scraper.driver.find_element_by_tag_name('html')\n",
    "        html.send_keys(Keys.END)\n",
    "        \n",
    "    time.sleep(2)\n",
    "    \n",
    "    for _ in range(25):\n",
    "        html = sc_scraper.driver.find_element_by_tag_name('html')\n",
    "        html.send_keys(Keys.END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc_scraper = SC_Scraper(debug = False, platform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc_scraper.to_url(\"https://www.bilibili.com/bangumi/play/ep302023\")\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_block = sc_scraper.driver.find_element_by_xpath('//*[@id=\"eplist_module\"]/div[2]/ul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_episode_list = episode_block.find_elements_by_xpath(\"./*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_episode_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_scraper.change_header(['user_id','user_homepage','level','timestamp','text','like','episode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________\n",
      "4\n",
      "1/84\n",
      "2/84\n",
      "3/84\n",
      "4/84\n",
      "5/84\n",
      "6/84\n",
      "7/84\n",
      "8/84\n",
      "9/84\n",
      "10/84\n",
      "11/84\n",
      "12/84\n",
      "13/84\n",
      "14/84\n",
      "15/84\n",
      "16/84\n",
      "17/84\n",
      "18/84\n",
      "19/84\n",
      "20/84\n",
      "21/84\n",
      "22/84\n",
      "23/84\n",
      "24/84\n",
      "25/84\n",
      "26/84\n",
      "27/84\n",
      "28/84\n",
      "29/84\n",
      "30/84\n",
      "31/84\n",
      "32/84\n",
      "33/84\n",
      "34/84\n",
      "35/84\n",
      "36/84\n",
      "37/84\n",
      "38/84\n",
      "39/84\n",
      "40/84\n",
      "41/84\n",
      "42/84\n",
      "43/84\n",
      "44/84\n",
      "45/84\n",
      "46/84\n",
      "47/84\n",
      "48/84\n",
      "49/84\n",
      "50/84\n",
      "51/84\n",
      "52/84\n",
      "53/84\n",
      "54/84\n",
      "55/84\n",
      "56/84\n",
      "57/84\n",
      "58/84\n",
      "59/84\n",
      "60/84\n",
      "61/84\n",
      "62/84\n",
      "63/84\n",
      "64/84\n",
      "65/84\n",
      "66/84\n",
      "67/84\n",
      "68/84\n",
      "69/84\n",
      "70/84\n",
      "71/84\n",
      "72/84\n",
      "73/84\n",
      "74/84\n",
      "75/84\n",
      "76/84\n",
      "77/84\n",
      "78/84\n",
      "79/84\n",
      "80/84\n",
      "81/84\n",
      "82/84\n",
      "83/84\n",
      "84/84\n",
      "________________\n",
      "5\n",
      "1/96\n",
      "2/96\n",
      "3/96\n",
      "4/96\n",
      "5/96\n",
      "6/96\n",
      "7/96\n",
      "8/96\n",
      "9/96\n",
      "10/96\n",
      "11/96\n",
      "12/96\n",
      "13/96\n",
      "14/96\n",
      "15/96\n",
      "16/96\n",
      "17/96\n",
      "18/96\n",
      "19/96\n",
      "20/96\n",
      "21/96\n",
      "22/96\n",
      "23/96\n",
      "24/96\n",
      "25/96\n",
      "26/96\n",
      "27/96\n",
      "28/96\n",
      "29/96\n",
      "30/96\n",
      "31/96\n",
      "32/96\n",
      "33/96\n",
      "34/96\n",
      "35/96\n",
      "36/96\n",
      "37/96\n",
      "38/96\n",
      "39/96\n",
      "40/96\n",
      "41/96\n",
      "42/96\n",
      "43/96\n",
      "44/96\n",
      "45/96\n",
      "46/96\n",
      "47/96\n",
      "48/96\n",
      "49/96\n",
      "50/96\n",
      "51/96\n",
      "52/96\n",
      "53/96\n",
      "54/96\n",
      "55/96\n",
      "56/96\n",
      "57/96\n",
      "58/96\n",
      "59/96\n",
      "60/96\n",
      "61/96\n",
      "62/96\n",
      "63/96\n",
      "64/96\n",
      "65/96\n",
      "66/96\n",
      "67/96\n",
      "68/96\n",
      "69/96\n",
      "70/96\n",
      "71/96\n",
      "72/96\n",
      "73/96\n",
      "74/96\n",
      "75/96\n",
      "76/96\n",
      "77/96\n",
      "78/96\n",
      "79/96\n",
      "80/96\n",
      "81/96\n",
      "82/96\n",
      "83/96\n",
      "84/96\n",
      "85/96\n",
      "86/96\n",
      "87/96\n",
      "88/96\n",
      "89/96\n",
      "90/96\n",
      "91/96\n",
      "92/96\n",
      "93/96\n",
      "94/96\n",
      "95/96\n",
      "96/96\n",
      "________________\n",
      "6\n",
      "1/72\n",
      "2/72\n",
      "3/72\n",
      "4/72\n",
      "5/72\n",
      "6/72\n",
      "7/72\n",
      "8/72\n",
      "9/72\n",
      "10/72\n",
      "11/72\n",
      "12/72\n",
      "13/72\n",
      "14/72\n",
      "15/72\n",
      "16/72\n",
      "17/72\n",
      "18/72\n",
      "19/72\n",
      "20/72\n",
      "21/72\n",
      "22/72\n",
      "23/72\n",
      "24/72\n",
      "25/72\n",
      "26/72\n",
      "27/72\n",
      "28/72\n",
      "29/72\n",
      "30/72\n",
      "31/72\n",
      "32/72\n",
      "33/72\n",
      "34/72\n",
      "35/72\n",
      "36/72\n",
      "37/72\n",
      "38/72\n",
      "39/72\n",
      "40/72\n",
      "41/72\n",
      "42/72\n",
      "43/72\n",
      "44/72\n",
      "45/72\n",
      "46/72\n",
      "47/72\n",
      "48/72\n",
      "49/72\n",
      "50/72\n",
      "51/72\n",
      "52/72\n",
      "53/72\n",
      "54/72\n",
      "55/72\n",
      "56/72\n",
      "57/72\n",
      "58/72\n",
      "59/72\n",
      "60/72\n",
      "61/72\n",
      "62/72\n",
      "63/72\n",
      "64/72\n",
      "65/72\n",
      "66/72\n",
      "67/72\n",
      "68/72\n",
      "69/72\n",
      "70/72\n",
      "71/72\n",
      "72/72\n",
      "________________\n",
      "7\n",
      "1/75\n",
      "2/75\n",
      "3/75\n",
      "4/75\n",
      "5/75\n",
      "6/75\n",
      "7/75\n",
      "8/75\n",
      "9/75\n",
      "10/75\n",
      "11/75\n",
      "12/75\n",
      "13/75\n",
      "14/75\n",
      "15/75\n",
      "16/75\n",
      "17/75\n",
      "18/75\n",
      "19/75\n",
      "20/75\n",
      "21/75\n",
      "22/75\n",
      "23/75\n",
      "24/75\n",
      "25/75\n",
      "26/75\n",
      "27/75\n",
      "28/75\n",
      "29/75\n",
      "30/75\n",
      "31/75\n",
      "32/75\n",
      "33/75\n",
      "34/75\n",
      "35/75\n",
      "36/75\n",
      "37/75\n",
      "38/75\n",
      "39/75\n",
      "40/75\n",
      "41/75\n",
      "42/75\n",
      "43/75\n",
      "44/75\n",
      "45/75\n",
      "46/75\n",
      "47/75\n",
      "48/75\n",
      "49/75\n",
      "50/75\n",
      "51/75\n",
      "52/75\n",
      "53/75\n",
      "54/75\n",
      "55/75\n",
      "56/75\n",
      "57/75\n",
      "58/75\n",
      "59/75\n",
      "60/75\n",
      "61/75\n",
      "62/75\n",
      "63/75\n",
      "64/75\n",
      "65/75\n",
      "66/75\n",
      "67/75\n",
      "68/75\n",
      "69/75\n",
      "70/75\n",
      "71/75\n",
      "72/75\n",
      "73/75\n",
      "74/75\n",
      "75/75\n",
      "________________\n",
      "8\n",
      "1/60\n",
      "2/60\n",
      "3/60\n",
      "4/60\n",
      "5/60\n",
      "6/60\n",
      "7/60\n",
      "8/60\n",
      "9/60\n",
      "10/60\n",
      "11/60\n",
      "12/60\n",
      "13/60\n",
      "14/60\n",
      "15/60\n",
      "16/60\n",
      "17/60\n",
      "18/60\n",
      "19/60\n",
      "20/60\n",
      "21/60\n",
      "22/60\n",
      "23/60\n",
      "24/60\n",
      "25/60\n",
      "26/60\n",
      "27/60\n",
      "28/60\n",
      "29/60\n",
      "30/60\n",
      "31/60\n",
      "32/60\n",
      "33/60\n",
      "34/60\n",
      "35/60\n",
      "36/60\n",
      "37/60\n",
      "38/60\n",
      "39/60\n",
      "40/60\n",
      "41/60\n",
      "42/60\n",
      "43/60\n",
      "44/60\n",
      "45/60\n",
      "46/60\n",
      "47/60\n",
      "48/60\n",
      "49/60\n",
      "50/60\n",
      "51/60\n",
      "52/60\n",
      "53/60\n",
      "54/60\n",
      "55/60\n",
      "56/60\n",
      "57/60\n",
      "58/60\n",
      "59/60\n",
      "60/60\n"
     ]
    }
   ],
   "source": [
    "for episode_idx in range(4,9):\n",
    "    print(\"________________\")\n",
    "    print(episode_idx)\n",
    "\n",
    "    single_episode_list[episode_idx].click()\n",
    "    time.sleep(3)\n",
    "    scroll()\n",
    "    time.sleep(10)\n",
    "    page_number = int(sc_scraper.driver.find_element_by_xpath('//*[@id=\"comment_module\"]/div[2]/div/div[2]/div[2]/span[1]').text.replace(\"共\",'').replace(\"页\",''))\n",
    "\n",
    "    comment_writer = sc_scraper.csv_writer(path='data/', outfile=f'v0_episode_{str(episode_idx)}.csv')\n",
    "\n",
    "    for tmp_page in range(page_number):\n",
    "        print(f\"{tmp_page+1}/{page_number}\")\n",
    "        scroll()\n",
    "        page_inbox = sc_scraper.driver.find_element_by_class_name('page-jump').find_element_by_xpath('./input')\n",
    "        page_inbox.clear()\n",
    "        page_inbox.send_keys(int(tmp_page+1))\n",
    "        page_inbox.send_keys(Keys.RETURN)\n",
    "        scroll()\n",
    "        time.sleep(1)\n",
    "\n",
    "        comment_block = sc_scraper.driver.find_element_by_class_name('comment-list').find_elements_by_xpath(\"./*\")\n",
    "        for idx, tmp_comment in enumerate(comment_block):\n",
    "            text = tmp_comment.find_elements_by_class_name(\"text\")[0].text\n",
    "            try:\n",
    "                user_homepage = tmp_comment.find_elements_by_class_name(\"user\")[0].find_elements_by_xpath(\"./*\")[0].get_attribute('href')\n",
    "            except:\n",
    "                user_homepage = 'None'\n",
    "            try:\n",
    "                if user_homepage != 'None':\n",
    "                    user_id = user_homepage.split(\"/\")[-1]\n",
    "                else: \n",
    "                    user_id = 'None'\n",
    "            except:\n",
    "                user_id = 'None'\n",
    "            try:   \n",
    "                level = tmp_comment.find_elements_by_class_name(\"user\")[0].find_elements_by_class_name(\"level-link\")[0].find_elements_by_xpath(\"./*\")[0].get_attribute('class')\n",
    "            except:\n",
    "                level = 'None'\n",
    "            timestamp = tmp_comment.find_elements_by_class_name(\"info\")[0].find_elements_by_class_name(\"time\")[0].text\n",
    "            like = tmp_comment.find_elements_by_class_name(\"info\")[0].find_elements_by_class_name(\"like\")[0].text\n",
    "\n",
    "            row_val = [user_id,user_homepage,level,timestamp,text,like,episode_idx]\n",
    "            comment_writer.writerow(row_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
